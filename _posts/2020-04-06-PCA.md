---
layout: post
title: Principal Component Analysis
subtitle: Dimensoinality Reduction
gh-repo: daattali/beautiful-jekyll
comments: true
---

### Ever wondered how to reduce higher dimensional data to a lower-dimensional data to visualize it better?


Possible to interpret this image?

<img src="/img/cube.png">{: .center-block :}

It's a 10-D cube and it's hard to visualize this figure.

Humans can only visualize 1D, 2D, and 3D spaces. Even though we live in a 3D world, our eyes can see only in 2D. Our retina has a 2D surface and it's the 2D projection of 3D objects we are looking at. So, in order to get a clear view of N-dimensional data, one needs to have an (N-1)-dimensional retina surface and would see the N-dimensional objects as (N-1)-dimensional projections. Practically, this ain't possible.

**"Visualize this thing that you want, see it, feel it, believe in it. Make your mental blueprint, and begin to build." -RobertÂ Collier**


Principal Component Analysis generally termed as PCA, is a handy solution for solving the dimensionality problem. Following the above quote by Robert Collier, one has to visualize and understand the data prior to building a model.

PCA helps to remove the features with less variance. Consider a simple example of dataset with only two features. One of the features representing the blackness of hair and the other feature being the weight of an individual. All the data points are from Indians.

<img src="/img/initial.PNG">{: .center-block :}

In the above image, the variance over the weight axis is high compared to the variance over the x-axis which is very less. Suppose, the idea is to convert 2D to 1D, then the x-axis can be eliminated. All the points over the y-axis can be plotted on a single axis(1D). Through this way, the direction with maximal variance is preserved without the information being lost.

## What if the features are equally distributred over both the axis? 
<img src="/img/initial1.PNG">{: .center-block :}

In the above figure, the variance on X-axis and Y-axis is similar. It is not a good idea to eliminate one of the axis in order to reduce it to 1D. Instead draw a straight line(X') through the points where the variance is high, and a line(Y') perpendicular to X'.

# Dimensionality Reduction of [MNIST](http://yann.lecun.com/exdb/mnist/) Data:

Download the dataset from [here](https://www.kaggle.com/c/digit-recognizer/data).

<img src="/img/pca/pca1.PNG">

Load the mnist data, and remove the labels from the original dataset.

Datapoint with 784 features would look like this:

<img src="/img/pca/pca2.PNG">

Use only 20000 points for faster computation, and [standardize](https://scikit-learn.org/stable/modules/preprocessing.html) the data.

<img src="/img/pca/pca3.PNG">

PCA is imported from [sklearn.decomposition](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html), considering all the 784 dimensions.

<img src="/img/pca/pca4.PNG">

Graph is obtained with total percentage of information explanined corresponding to number of dimensions.

<img src="/img/pca/pca5.PNG">{: .center-block :}

From the above diagram, 20% of the information is explained when converted to 10-D. If 784-D trasnformed to 300-D, alomst 95% of the information is retained. 


# 2D visualization of MNIST Data

Let's visualize the 784-D MNIST dataset in 2-D to get a clear understanding of PCA.

First, covariance matrix will be calculated by multiplying the data with the transpose of same data.
co-variance(S) = x<sup>T</sup>*x

<img src="/img/pca/pca6.PNG">

Then, eigen values and eigen vectors are formulated from the co-variance matrix using eigh function imported from [scipy.linalg](https://docs.scipy.org/doc/scipy/reference/generated/scipy.linalg.eigh.html). As the top 2 valued eigen values and vectors are needed for 2-d and these values will be arranged in ascending order, we would consider only 782 and 783 values.

<img src="/img/pca/pca7.PNG">

Finally, the new data with 2 dimensions will be obtained by multiplying the vectors matrix with the original data and the labels will be stacked with the new data, indicating the size (20000,3). 

<img src="/img/pca/pca8.PNG">

## 2-D visualization

<img src="/img/pca/pca9.PNG">{: .center-block :}

From the figure, one cannot differentiate the numbers as only 10% of the information is explained in 2-D.



Dataset and the whole code for the same is available [here](https://github.com/yeshwanth16/PCA-MNIST).
