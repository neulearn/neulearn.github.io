---
layout: post
title: t-SNE
subtitle: Dimensoinality Reduction
gh-repo: daattali/beautiful-jekyll
comments: true
---

State of Art dimensional reduction technique for visualization.

t-SNE stands for T-distributed Stochastic Neighbor Embedding(hard to remember though:< ) developed by Laurens van der Maaten and Geoffrey Hinton. It is non-linear reduction technique which helps in embedding high dimensional data in 2-D or 3-D space for visualization.

Looking at the write-up on Principal Component Analysis, PCA was not doing well except for '0' with the MNIST dataset in 2-D. It was hard to visualize and distinguish between the other numbers. PCA tries to preserve the global structure of data whereas t-SNE preserves local structure too.

<img src="/img/pca/pca9.PNG">{: .center-block :}
 
## Understanding Neighbourhood and Embedding

<img src="/img/tsne/tsne1.PNG">{: .center-block :}

In the above figure, consider a point as x<sub>i</sub> in the red cluster. The cluster represents the neighbourhood of x<sub>i</sub>(N(x<sub>i</sub>)). 

#### N(x<sub>i</sub>)={ x<sub>j</sub> such that x<sub>i</sub> and x<sub>j</sub> are geometrically close.
Here, N(x<sub>i</sub>) contains all the points present in the red cluster as they are close to x<sub>i</sub>.

For every point in higher dimensional space, a new point will be created in the lower dimensional space by preserving the distance between points in the higher dimensional space, known as Embedding.

<img src="/img/tsne/tsne2.PNG">{: .center-block :}

Here, all the points in respective clusters have been embedded on to 1-D space. The intra-cluster distances i.e distance between points in the clusters are same, but inter-cluster distances are not preserved. 

## 2-D Visualisation of MNIST data

t-SNE is imported from [sklearn.manifold](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html).

<img src="/img/tsne/tsne3.PNG">

Perplexity defines the number of points closer to the selected point in the neighbourhood. Default iterations size=1000.

###### With perplexity=20.
<img src="/img/tsne/tsne4.PNG">{: .center-block :}


<img src="/img/tsne/tsne5.PNG">

###### With perplexity=60.
<img src="/img/tsne/tsne6.PNG">{: .center-block :}

<img src="/img/tsne/tsne7.PNG">

###### With perplexity=50, iterations=5000.
<img src="/img/tsne/tsne8.PNG">{: .center-block :}

<img src="/img/tsne/tsne9.PNG">

###### With perplexity=4.
<img src="/img/tsne/tsne10.PNG">{: .center-block :}

In the above plot, the labels cannot be differentiated as the perplexity is very less.



