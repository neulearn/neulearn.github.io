---
layout: post
title: Bert vs StackOverflow Bert
subtitle: BERT
bigimg: /bert/bert.jpg
---
# Is there anyone working predominantly in NLP who doesn't know what BERT is?? I highly doubt that.

*Don't confuse yourself thinking that I am going to talk about the Sesame Street TV series. The image for Bert was taken from this TV series where there were two muppets named Bert and Ernie. :)

<img src="/bert/sesame.jpg">{: .center-block :}

# What is BERT?

Bidirectional Encoder Representations from Transformers commonly called as BERT, is a pre-trained NLP model developed by Google. BERT has achieved SOTA performance on multiple NLP tasks. Unlike W2V, Glove word embeddings, BERT generates contextual word representations. Transformers are the basic building blocks of BERT, BERT is built over the enocders section of Transformers. BERT small and LARGE has 12, 24 encoders respectively.

<img src="/bert/bert_small.png">{: .center-block :}
