---
layout: post
title: Bert vs StackOverflow Bert
subtitle: BERT
bigimg: /bert/bert.jpg
---
# Is there anyone working predominantly in NLP who doesn't know what BERT is?? I highly doubt that.

*Don't confuse yourself thinking that I am going to talk about the Sesame Street TV series. The image for Bert was taken from this TV series where there were two muppets named Bert and Ernie. :)

<img src="/bert/sesame.jpg">{: .center-block :}

# What is BERT?

Bidirectional Encoder Representations from Transformers commonly called as BERT, is a pre-trained NLP model developed by Google. BERT has achieved SOTA performance on multiple NLP tasks. Unlike W2V, Glove word embeddings, BERT generates contextual word representations. Transformers are the basic building blocks of BERT, BERT is built over the enocders section of Transformers. BERT small and LARGE has 12, 24 encoders respectively.

<img src="/bert/bert_small.PNG">{: .center-block :}


Pre-training BERT from scratch would be an onerous as we would need huge corpus of data and lots of GPUs. Huggingface community has provided more than 1,500 pre-trained models. 
We will be using bert_base_uncased model which was trained on lower-cased English text containing 12-layer, 768-hidden, 12-heads, 110M parameters.

On the other hand, there is a BERT model trained on 152 million sentences from the StackOverflow's 10 year archive - [BERTOverflow](https://huggingface.co/jeniya/BERTOverflow_stackoverflow_github). We will be comparing BERTOverflow with generic BERT model.


